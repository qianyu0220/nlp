{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spaCy](https://spacy.io/) is one amongst many great industry used NLP tools.\n",
    "\n",
    "spaCy's strength is it's ability to use pretrained models for a variety of NLP\n",
    "tasks. Users can even provide their own labeled language data to train &\n",
    "fine-tune models in spaCy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we'll use spaCy's already trained pipeline to perform\n",
    "lemmatization on text as well as do NLP tasks like named entity recognition \n",
    "(NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because spaCy has NLP models trained with relevant language data, the first step\n",
    "is to download these models so they can be utilized.\n",
    "\n",
    "The line below uses spaCy to download the \n",
    "[en_core_web_sm](https://spacy.io/models/en#en_core_web_sm) pipeline. This is \n",
    "ideal since it's relatively small to download (at ~12 MB) and is optimized to\n",
    "use the CPU. It also has lemmatizer and NER components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/venv/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: setuptools in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
      "Requirement already satisfied: jinja2 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/venv/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/venv/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/venv/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/venv/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/venv/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /opt/venv/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the `spacy` module and load the trained [pipeline](https://spacy.io/usage/spacy-101#pipelines).\n",
    "\n",
    "\n",
    "![](https://spacy.io/images/pipeline.svg)\n",
    "\n",
    "Below is the standard way the pipeline is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Base Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the text below and use the built-in tokenizer to tokenize the text to get\n",
    "a list of all the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Smith graduated from the University of Washington. He later started an analytics firm called Lux, which catered to enterprise customers.\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    'Dr. Smith graduated from the University of Washington. '\n",
    "    'He later started an analytics firm called Lux, which catered to enterprise customers.'\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the tokens using spaCy's – it only has to be iterated over\n",
    "tokens = nlp.tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Smith\n",
      "graduated\n",
      "from\n",
      "the\n",
      "University\n",
      "of\n",
      "Washington\n",
      ".\n",
      "He\n",
      "later\n",
      "started\n",
      "an\n",
      "analytics\n",
      "firm\n",
      "called\n",
      "Lux\n",
      ",\n",
      "which\n",
      "catered\n",
      "to\n",
      "enterprise\n",
      "customers\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Check you can iterate over the tokens\n",
    "for token in tokens:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the Matrix. Are the human people the ones who started the war? Is AI a bad thing ?\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    'The first time you see The Second Renaissance it may look boring. '\n",
    "    'Look at it at least twice and definitely watch part 2. '\n",
    "    'It will change your view of the Matrix. '\n",
    "    'Are the human people the ones who started the war? Is AI a bad thing ?'\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs: list[tuple[str, str]] = []\n",
    "# TODO: Use spaCy to compare the tokens before and after lemmatization\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    orig = token.text\n",
    "    lemma = token.lemma_\n",
    "    if orig != lemma:\n",
    "        diff = (orig, lemma)\n",
    "        diffs.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 differences between the tokens before and after lemmatization\n",
      "The the\n",
      "The the\n",
      "Look look\n",
      "It it\n",
      "Are be\n",
      "ones one\n",
      "started start\n",
      "Is be\n"
     ]
    }
   ],
   "source": [
    "# Check the differences between your tokens\n",
    "print(f'There are {len(diffs)} differences between the tokens before and after lemmatization')\n",
    "\n",
    "for orig, lemma in diffs:\n",
    "    print(orig, lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now closer at the the tokens, specifically at the parts of speech for each\n",
    "token, according to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Smith graduated from the University of Washington. He later started an analytics firm called Lux, which catered to enterprise customers.\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    'Dr. Smith graduated from the University of Washington. '\n",
    "    'He later started an analytics firm called Lux, which catered to enterprise customers.'\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_groupings: dict[str, list[str]] = {}\n",
    "# TODO: Group the tokens by parts of speech\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    pos_tag = token.pos_\n",
    "    if pos_tag not in pos_groupings:\n",
    "        pos_groupings[pos_tag] = list()\n",
    "    pos_groupings[pos_tag].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN\n",
      "\t [Dr., Smith, University, Washington, Lux]\n",
      "VERB\n",
      "\t [graduated, started, called, catered]\n",
      "ADP\n",
      "\t [from, of, to]\n",
      "DET\n",
      "\t [the, an]\n",
      "PUNCT\n",
      "\t [., ,, .]\n",
      "PRON\n",
      "\t [He, which]\n",
      "ADV\n",
      "\t [later]\n",
      "NOUN\n",
      "\t [analytics, firm, enterprise, customers]\n"
     ]
    }
   ],
   "source": [
    "# Check your part of speech groupings\n",
    "for pos_tag, tokens in pos_groupings.items():\n",
    "    print(pos_tag)\n",
    "    print('\\t', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First determine what are the labels used by the model we loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: List the different entity labels used by the spaCy pipeline being used\n",
    "nlp.get_pipe(\"ner\").labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, take the following text and list out all the tokens that have an\n",
    "associated entity label.\n",
    "Also list out the type of entity it is according to the\n",
    "pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the Matrix. Are the human people the ones who started the war? Is AI a bad thing ?\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    'The first time you see The Second Renaissance it may look boring. '\n",
    "    'Look at it at least twice and definitely watch part 2. '\n",
    "    'It will change your view of the Matrix. '\n",
    "    'Are the human people the ones who started the war? Is AI a bad thing ?'\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: first\n",
      "\t Entity: ORDINAL\n",
      "Text: The Second Renaissance\n",
      "\t Entity: ORG\n",
      "Text: 2\n",
      "\t Entity: CARDINAL\n",
      "Text: the Matrix\n",
      "\t Entity: WORK_OF_ART\n"
     ]
    }
   ],
   "source": [
    "# TODO: List only the tokens that are entities along with their labels\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc.ents:\n",
    "    text = token.text\n",
    "    entity = token.label_\n",
    "    print(f'Text: {text}\\n\\t Entity: {entity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---------- Alternative ----------\n",
      "Text: first\n",
      "\t Entity: ORDINAL\n",
      "Text: The\n",
      "\t Entity: ORG\n",
      "Text: Second\n",
      "\t Entity: ORG\n",
      "Text: Renaissance\n",
      "\t Entity: ORG\n",
      "Text: 2\n",
      "\t Entity: CARDINAL\n",
      "Text: the\n",
      "\t Entity: WORK_OF_ART\n",
      "Text: Matrix\n",
      "\t Entity: WORK_OF_ART\n"
     ]
    }
   ],
   "source": [
    "print('\\n', 10*'-', 'Alternative', '-'*10)\n",
    "for token in doc:\n",
    "    text = token.text\n",
    "    entity = token.ent_type_\n",
    "    if entity:\n",
    "        print(f'Text: {text}\\n\\t Entity: {entity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
